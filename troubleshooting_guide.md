# LLMå¤§æ¨¡å‹éƒ¨ç½²æ•…éšœæ’é™¤æŒ‡å—

## ğŸ” å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

### 1. æ¨¡å‹ä¸‹è½½é—®é¢˜

#### 1.1 HuggingFaceè®¤è¯å¤±è´¥
**é—®é¢˜**: `401 Unauthorized` æˆ– `Authentication required`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³•1: ä½¿ç”¨huggingface-cliç™»å½•
huggingface-cli login
# è¾“å…¥ä½ çš„token: hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# æ–¹æ³•2: è®¾ç½®ç¯å¢ƒå˜é‡
export HUGGINGFACE_HUB_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# æ–¹æ³•3: åœ¨ä»£ç ä¸­ç›´æ¥ä½¿ç”¨token
python model_download.py download --model qwen2.5-32b --token YOUR_TOKEN

# éªŒè¯ç™»å½•çŠ¶æ€
huggingface-cli whoami
```

#### 1.2 ç½‘ç»œè¿æ¥é—®é¢˜
**é—®é¢˜**: ä¸‹è½½é€Ÿåº¦æ…¢æˆ–è¿æ¥è¶…æ—¶

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
ping huggingface.co

# ä½¿ç”¨ä»£ç†ï¼ˆå¦‚æœæœ‰ï¼‰
export https_proxy=http://your-proxy:port
export http_proxy=http://your-proxy:port

# ä½¿ç”¨é•œåƒç«™ç‚¹
export HF_ENDPOINT=https://hf-mirror.com

# æ–­ç‚¹ç»­ä¼ 
python model_download.py download --model qwen2.5-32b --resume
```

#### 1.3 ç£ç›˜ç©ºé—´ä¸è¶³
**é—®é¢˜**: `No space left on device`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
sudo apt autoremove
sudo apt autoclean

# ç§»åŠ¨æ¨¡å‹åˆ°å…¶ä»–åˆ†åŒº
sudo mkdir -p /mnt/large_disk/models
sudo chown $USER:$USER /mnt/large_disk/models
# ä¿®æ”¹config.pyä¸­çš„MODEL_PATH
```

### 2. æœåŠ¡å¯åŠ¨é—®é¢˜

#### 2.1 GPUå†…å­˜ä¸è¶³
**é—®é¢˜**: `CUDA out of memory` æˆ– `RuntimeError: CUDA error: out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä¿®æ”¹config.pyï¼Œå¯ç”¨æ›´æ¿€è¿›çš„é‡åŒ–
USE_QUANTIZATION = True
QUANTIZATION_BITS = 4
GPU_MEMORY_FRACTION = 0.8  # é™ä½GPUå†…å­˜ä½¿ç”¨ç‡

# æˆ–è€…ä½¿ç”¨æ›´å°çš„æ¨¡å‹
MODEL_NAME = "Qwen/Qwen2.5-14B-Instruct"  # ä»32Bæ”¹ä¸º14B
```

#### 2.2 æ¨¡å‹æ–‡ä»¶æŸå
**é—®é¢˜**: `FileNotFoundError` æˆ–æ¨¡å‹åŠ è½½å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
python model_download.py verify --model qwen2.5-32b

# é‡æ–°ä¸‹è½½æ¨¡å‹
rm -rf /data/models/Qwen2.5-32B-Instruct
python model_download.py download --model qwen2.5-32b

# æ£€æŸ¥æ–‡ä»¶æƒé™
sudo chown -R $USER:$USER /data/models
```

#### 2.3 ç«¯å£è¢«å ç”¨
**é—®é¢˜**: `Address already in use` æˆ– `Port 8000 is already in use`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
sudo netstat -tlnp | grep 8000
sudo lsof -i :8000

# æ€æ­»å ç”¨è¿›ç¨‹
sudo kill -9 <PID>

# æˆ–è€…ä¿®æ”¹ç«¯å£
# ç¼–è¾‘config.py
PORT = 8001  # æ”¹ä¸ºå…¶ä»–ç«¯å£
```

#### 2.4 ä¾èµ–åŒ…ç‰ˆæœ¬å†²çª
**é—®é¢˜**: `ImportError` æˆ–ç‰ˆæœ¬ä¸å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡æ–°åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
rm -rf venv
python3 -m venv venv
source venv/bin/activate

# é‡æ–°å®‰è£…ä¾èµ–
pip install --upgrade pip
pip install -r requirements.txt

# å¦‚æœä»æœ‰é—®é¢˜ï¼Œå°è¯•å›ºå®šç‰ˆæœ¬
pip install torch==2.1.2 transformers==4.44.2
```

### 3. è¿è¡Œæ—¶é—®é¢˜

#### 3.1 æ¨ç†é€Ÿåº¦æ…¢
**é—®é¢˜**: å“åº”æ—¶é—´è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä¼˜åŒ–é…ç½®
# config.py
MAX_TOKENS = 1024  # å‡å°‘æœ€å¤§tokenæ•°
BATCH_SIZE = 1     # å‡å°‘æ‰¹å¤„ç†å¤§å°
USE_QUANTIZATION = True  # å¯ç”¨é‡åŒ–

# ä½¿ç”¨VLLMå¼•æ“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
python vllm_server.py  # è€Œä¸æ˜¯model_server.py
```

#### 3.2 å†…å­˜æ³„æ¼
**é—®é¢˜**: é•¿æ—¶é—´è¿è¡Œåå†…å­˜ä½¿ç”¨æŒç»­å¢é•¿

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
watch -n 1 'free -h && nvidia-smi'

# å®šæœŸé‡å¯æœåŠ¡
sudo systemctl restart llm-server

# è®¾ç½®å†…å­˜é™åˆ¶
# åœ¨systemdæœåŠ¡æ–‡ä»¶ä¸­æ·»åŠ 
Environment=MALLOC_TRIM_THRESHOLD_=100000
```

#### 3.3 APIè¯·æ±‚è¶…æ—¶
**é—®é¢˜**: å®¢æˆ·ç«¯è¯·æ±‚è¶…æ—¶

**è§£å†³æ–¹æ¡ˆ**:
```python
# å¢åŠ å®¢æˆ·ç«¯è¶…æ—¶æ—¶é—´
client = LLMAPIClient("http://server:8000", timeout=600)  # 10åˆ†é’Ÿ

# æˆ–è€…ä½¿ç”¨æµå¼è¾“å‡º
for chunk in client.chat(messages, stream=True):
    print(chunk, end="", flush=True)
```

### 4. ç³»ç»Ÿçº§é—®é¢˜

#### 4.1 CUDAç‰ˆæœ¬ä¸åŒ¹é…
**é—®é¢˜**: `CUDA runtime error` æˆ–ç‰ˆæœ¬ä¸å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥CUDAç‰ˆæœ¬
nvidia-smi
nvcc --version

# å®‰è£…åŒ¹é…çš„PyTorchç‰ˆæœ¬
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

#### 4.2 ç³»ç»Ÿèµ„æºä¸è¶³
**é—®é¢˜**: ç³»ç»Ÿå“åº”ç¼“æ…¢æˆ–æœåŠ¡ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
htop
iostat -x 1
nvidia-smi -l 1

# ä¼˜åŒ–ç³»ç»Ÿé…ç½®
# å¢åŠ swapç©ºé—´
sudo fallocate -l 16G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# è°ƒæ•´ç³»ç»Ÿå‚æ•°
echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf
echo 'vm.dirty_ratio=15' | sudo tee -a /etc/sysctl.conf
```

### 5. ç½‘ç»œå’Œå®‰å…¨é—®é¢˜

#### 5.1 é˜²ç«å¢™é˜»æ­¢è®¿é—®
**é—®é¢˜**: æ— æ³•ä»å¤–éƒ¨è®¿é—®API

**è§£å†³æ–¹æ¡ˆ**:
```bash
# Ubuntu/Debian
sudo ufw allow 8000/tcp
sudo ufw status

# CentOS/RHEL
sudo firewall-cmd --permanent --add-port=8000/tcp
sudo firewall-cmd --reload
sudo firewall-cmd --list-ports

# æ£€æŸ¥æœåŠ¡ç»‘å®š
netstat -tlnp | grep 8000
# åº”è¯¥æ˜¾ç¤º 0.0.0.0:8000 è€Œä¸æ˜¯ 127.0.0.1:8000
```

#### 5.2 SSL/TLSè¯ä¹¦é—®é¢˜
**é—®é¢˜**: HTTPSè®¿é—®å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç”Ÿæˆè‡ªç­¾åè¯ä¹¦ï¼ˆæµ‹è¯•ç”¨ï¼‰
openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes

# é…ç½®Nginxåå‘ä»£ç†
# /etc/nginx/sites-available/llm-api
server {
    listen 443 ssl;
    server_name your-domain.com;
    
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    
    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### 6. æ—¥å¿—å’Œè°ƒè¯•

#### 6.1 æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
```bash
# ç³»ç»ŸæœåŠ¡æ—¥å¿—
sudo journalctl -u llm-server -f

# åº”ç”¨æ—¥å¿—
tail -f /var/log/llm_deployment.log

# å®æ—¶ç›‘æ§
watch -n 1 'tail -n 20 /var/log/llm_deployment.log'
```

#### 6.2 å¯ç”¨è°ƒè¯•æ¨¡å¼
```python
# config.py
LOG_LEVEL = "DEBUG"  # æ”¹ä¸ºDEBUGçº§åˆ«

# æˆ–è€…åœ¨ä»£ç ä¸­æ·»åŠ è°ƒè¯•ä¿¡æ¯
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### 6.3 æ€§èƒ½åˆ†æ
```bash
# GPUä½¿ç”¨æƒ…å†µ
nvidia-smi -l 1

# CPUå’Œå†…å­˜ä½¿ç”¨
htop

# ç½‘ç»œè¿æ¥
netstat -an | grep 8000

# ç£ç›˜I/O
iostat -x 1
```

### 7. å¸¸è§é”™è¯¯ä»£ç 

| é”™è¯¯ä»£ç  | å«ä¹‰ | è§£å†³æ–¹æ¡ˆ |
|---------|------|----------|
| 401 | è®¤è¯å¤±è´¥ | æ£€æŸ¥HuggingFace token |
| 403 | æƒé™ä¸è¶³ | æ£€æŸ¥æ–‡ä»¶æƒé™å’Œé˜²ç«å¢™ |
| 404 | æœåŠ¡æœªæ‰¾åˆ° | æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨ |
| 500 | æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ | æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿— |
| 503 | æœåŠ¡ä¸å¯ç”¨ | æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½å®Œæˆ |
| 504 | ç½‘å…³è¶…æ—¶ | å¢åŠ è¶…æ—¶æ—¶é—´æˆ–ä¼˜åŒ–æ¨¡å‹ |

### 8. é¢„é˜²æªæ–½

#### 8.1 å®šæœŸç»´æŠ¤
```bash
# åˆ›å»ºç»´æŠ¤è„šæœ¬
#!/bin/bash
# maintenance.sh

# æ¸…ç†æ—¥å¿—æ–‡ä»¶
find /var/log -name "*.log" -size +100M -delete

# é‡å¯æœåŠ¡
sudo systemctl restart llm-server

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# æ›´æ–°ç³»ç»Ÿ
sudo apt update && sudo apt upgrade -y
```

#### 8.2 ç›‘æ§è„šæœ¬
```bash
#!/bin/bash
# monitor.sh

while true; do
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    if ! curl -s http://localhost:8000/health > /dev/null; then
        echo "$(date): æœåŠ¡å¼‚å¸¸ï¼Œæ­£åœ¨é‡å¯..."
        sudo systemctl restart llm-server
    fi
    
    # æ£€æŸ¥GPUçŠ¶æ€
    gpu_usage=$(nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits)
    if [ "$gpu_usage" -gt 95 ]; then
        echo "$(date): GPUä½¿ç”¨ç‡è¿‡é«˜: ${gpu_usage}%"
    fi
    
    sleep 60
done
```

#### 8.3 å¤‡ä»½ç­–ç•¥
```bash
#!/bin/bash
# backup.sh

backup_dir="/backup/llm-deployment-$(date +%Y%m%d)"
mkdir -p $backup_dir

# å¤‡ä»½é…ç½®æ–‡ä»¶
cp config.py $backup_dir/
cp requirements.txt $backup_dir/
cp *.py $backup_dir/

# å¤‡ä»½æœåŠ¡é…ç½®
sudo cp /etc/systemd/system/llm-server.service $backup_dir/

# å‹ç¼©å¤‡ä»½
tar -czf $backup_dir.tar.gz $backup_dir
rm -rf $backup_dir

echo "å¤‡ä»½å®Œæˆ: $backup_dir.tar.gz"
```

### 9. è·å–å¸®åŠ©

å¦‚æœä»¥ä¸Šè§£å†³æ–¹æ¡ˆéƒ½æ— æ³•è§£å†³é—®é¢˜ï¼Œè¯·ï¼š

1. **æ”¶é›†ä¿¡æ¯**:
   ```bash
   # ç³»ç»Ÿä¿¡æ¯
   uname -a
   nvidia-smi
   python --version
   pip list | grep torch
   
   # æœåŠ¡çŠ¶æ€
   sudo systemctl status llm-server
   sudo journalctl -u llm-server --no-pager -l
   ```

2. **æ£€æŸ¥æ—¥å¿—**:
   - ç³»ç»Ÿæ—¥å¿—: `/var/log/syslog`
   - åº”ç”¨æ—¥å¿—: `/var/log/llm_deployment.log`
   - æœåŠ¡æ—¥å¿—: `sudo journalctl -u llm-server`

3. **ç¤¾åŒºæ”¯æŒ**:
   - GitHub Issues
   - æŠ€æœ¯è®ºå›
   - å®˜æ–¹æ–‡æ¡£

---

**æ³¨æ„**: æœ¬æŒ‡å—åŸºäºå¸¸è§é—®é¢˜æ•´ç†ï¼Œå…·ä½“é—®é¢˜å¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´è§£å†³æ–¹æ¡ˆã€‚
