#!/bin/bash

# è¿œç¨‹æœåŠ¡å™¨LLMå¤§æ¨¡å‹éƒ¨ç½²è„šæœ¬
# é€‚ç”¨äºUbuntu/CentOSç³»ç»Ÿ

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "=========================================="
echo "è¿œç¨‹æœåŠ¡å™¨LLMå¤§æ¨¡å‹éƒ¨ç½²è„šæœ¬"
echo "=========================================="

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_step() {
    echo -e "${BLUE}[STEP]${NC} $1"
}

# æ£€æŸ¥æ˜¯å¦ä¸ºrootç”¨æˆ·
check_root() {
    if [[ $EUID -eq 0 ]]; then
        log_error "è¯·ä¸è¦ä½¿ç”¨rootç”¨æˆ·è¿è¡Œæ­¤è„šæœ¬"
        log_info "è¯·ä½¿ç”¨æ™®é€šç”¨æˆ·è¿è¡Œï¼Œè„šæœ¬ä¼šåœ¨éœ€è¦æ—¶è¯·æ±‚sudoæƒé™"
        exit 1
    fi
}

# æ£€æµ‹æ“ä½œç³»ç»Ÿ
detect_os() {
    if [[ -f /etc/os-release ]]; then
        . /etc/os-release
        OS=$NAME
        VER=$VERSION_ID
    else
        log_error "æ— æ³•æ£€æµ‹æ“ä½œç³»ç»Ÿ"
        exit 1
    fi
    
    log_info "æ£€æµ‹åˆ°æ“ä½œç³»ç»Ÿ: $OS $VER"
}

# å®‰è£…ç³»ç»Ÿä¾èµ–
install_system_deps() {
    log_step "å®‰è£…ç³»ç»Ÿä¾èµ–..."
    
    if [[ "$OS" == *"Ubuntu"* ]] || [[ "$OS" == *"Debian"* ]]; then
        sudo apt update
        sudo apt install -y python3 python3-pip python3-venv git curl wget htop nvidia-utils-535
    elif [[ "$OS" == *"CentOS"* ]] || [[ "$OS" == *"Red Hat"* ]]; then
        sudo yum update -y
        sudo yum install -y python3 python3-pip git curl wget htop
    else
        log_warn "æœªè¯†åˆ«çš„æ“ä½œç³»ç»Ÿï¼Œè¯·æ‰‹åŠ¨å®‰è£…Python3å’Œç›¸å…³ä¾èµ–"
    fi
    
    log_info "ç³»ç»Ÿä¾èµ–å®‰è£…å®Œæˆ"
}

# æ£€æŸ¥ç¡¬ä»¶è¦æ±‚
check_hardware() {
    log_step "æ£€æŸ¥ç¡¬ä»¶è¦æ±‚..."
    
    # æ£€æŸ¥GPU
    if ! command -v nvidia-smi &> /dev/null; then
        log_error "æœªæ£€æµ‹åˆ°NVIDIA GPUæˆ–é©±åŠ¨æœªå®‰è£…"
        log_info "è¯·ç¡®ä¿å·²å®‰è£…NVIDIAé©±åŠ¨: https://developer.nvidia.com/cuda-downloads"
        exit 1
    fi
    
    # æ£€æŸ¥GPUå†…å­˜
    gpu_memory=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    if [[ $gpu_memory -lt 20000 ]]; then
        log_warn "GPUå†…å­˜å¯èƒ½ä¸è¶³ï¼Œå»ºè®®20GB+ï¼Œå½“å‰: ${gpu_memory}MB"
        log_info "å»ºè®®ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼Œå¦‚Qwen2.5-7Bæˆ–Qwen2.5-14B"
    else
        log_info "GPUå†…å­˜æ£€æŸ¥é€šè¿‡: ${gpu_memory}MB"
    fi
    
    # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
    total_memory=$(free -g | awk '/^Mem:/{print $2}')
    if [[ $total_memory -lt 16 ]]; then
        log_warn "ç³»ç»Ÿå†…å­˜å¯èƒ½ä¸è¶³ï¼Œå»ºè®®16GB+ï¼Œå½“å‰: ${total_memory}GB"
    else
        log_info "ç³»ç»Ÿå†…å­˜æ£€æŸ¥é€šè¿‡: ${total_memory}GB"
    fi
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    available_space=$(df -BG / | awk 'NR==2{print $4}' | sed 's/G//')
    if [[ $available_space -lt 100 ]]; then
        log_warn "ç£ç›˜ç©ºé—´å¯èƒ½ä¸è¶³ï¼Œå»ºè®®100GB+ï¼Œå½“å‰å¯ç”¨: ${available_space}GB"
    else
        log_info "ç£ç›˜ç©ºé—´æ£€æŸ¥é€šè¿‡: ${available_space}GB"
    fi
}

# è®¾ç½®Pythonç¯å¢ƒ
setup_python_env() {
    log_step "è®¾ç½®Pythonç¯å¢ƒ..."
    
    # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
    if [ ! -d "venv" ]; then
        python3 -m venv venv
        log_info "è™šæ‹Ÿç¯å¢ƒåˆ›å»ºå®Œæˆ"
    else
        log_info "è™šæ‹Ÿç¯å¢ƒå·²å­˜åœ¨"
    fi
    
    # æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
    source venv/bin/activate
    log_info "è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»"
    
    # å‡çº§pip
    pip install --upgrade pip
    log_info "pipå·²å‡çº§"
}

# å®‰è£…Pythonä¾èµ–
install_python_deps() {
    log_step "å®‰è£…Pythonä¾èµ–..."
    
    # å®‰è£…PyTorch (CUDAç‰ˆæœ¬)
    log_info "å®‰è£…PyTorch (CUDAç‰ˆæœ¬)..."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    
    # å®‰è£…å…¶ä»–ä¾èµ–
    if [ -f "requirements.txt" ]; then
        pip install -r requirements.txt
    else
        log_warn "requirements.txtæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå®‰è£…åŸºç¡€ä¾èµ–..."
        pip install transformers accelerate bitsandbytes fastapi uvicorn huggingface_hub
    fi
    
    log_info "Pythonä¾èµ–å®‰è£…å®Œæˆ"
}

# åˆ›å»ºç›®å½•ç»“æ„
create_directories() {
    log_step "åˆ›å»ºç›®å½•ç»“æ„..."
    
    # åˆ›å»ºæ¨¡å‹ç›®å½•
    sudo mkdir -p /data/models
    sudo chown -R $USER:$USER /data/models
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    sudo mkdir -p /var/log
    sudo chown -R $USER:$USER /var/log
    
    # åˆ›å»ºæœ¬åœ°æ—¥å¿—ç›®å½•
    mkdir -p logs
    
    log_info "ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ"
}

# HuggingFaceè®¤è¯
setup_huggingface_auth() {
    log_step "è®¾ç½®HuggingFaceè®¤è¯..."
    
    echo "è¯·é€‰æ‹©HuggingFaceè®¤è¯æ–¹å¼:"
    echo "1. ä½¿ç”¨huggingface-cliç™»å½•"
    echo "2. è®¾ç½®ç¯å¢ƒå˜é‡"
    echo "3. è·³è¿‡ï¼ˆç¨åæ‰‹åŠ¨è®¾ç½®ï¼‰"
    
    read -p "è¯·é€‰æ‹© (1-3): " auth_choice
    
    case $auth_choice in
        1)
            log_info "è¯·æŒ‰ç…§æç¤ºè¾“å…¥ä½ çš„HuggingFace Token:"
            huggingface-cli login
            ;;
        2)
            read -p "è¯·è¾“å…¥ä½ çš„HuggingFace Token: " hf_token
            echo "export HUGGINGFACE_HUB_TOKEN=\"$hf_token\"" >> ~/.bashrc
            export HUGGINGFACE_HUB_TOKEN="$hf_token"
            log_info "Tokenå·²è®¾ç½®åˆ°ç¯å¢ƒå˜é‡"
            ;;
        3)
            log_warn "è·³è¿‡HuggingFaceè®¤è¯è®¾ç½®"
            log_info "è¯·ç¨åæ‰‹åŠ¨è¿è¡Œ: huggingface-cli login"
            ;;
        *)
            log_warn "æ— æ•ˆé€‰æ‹©ï¼Œè·³è¿‡è®¤è¯è®¾ç½®"
            ;;
    esac
}

# ä¸‹è½½æ¨¡å‹
download_model() {
    log_step "å‡†å¤‡æ¨¡å‹ä¸‹è½½..."
    
    echo "è¯·é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹:"
    echo "1. Qwen2.5-7B-Instruct (çº¦14GBï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒ)"
    echo "2. Qwen2.5-14B-Instruct (çº¦28GBï¼Œå¹³è¡¡é€‰æ‹©)"
    echo "3. Qwen2.5-32B-Instruct (çº¦64GBï¼Œæ¨èA800)"
    echo "4. è·³è¿‡ä¸‹è½½"
    
    read -p "è¯·é€‰æ‹© (1-4): " model_choice
    
    case $model_choice in
        1)
            model_name="qwen2.5-7b"
            ;;
        2)
            model_name="qwen2.5-14b"
            ;;
        3)
            model_name="qwen2.5-32b"
            ;;
        4)
            log_info "è·³è¿‡æ¨¡å‹ä¸‹è½½"
            return
            ;;
        *)
            log_error "æ— æ•ˆé€‰æ‹©"
            return
            ;;
    esac
    
    log_info "å¼€å§‹ä¸‹è½½æ¨¡å‹: $model_name"
    
    if [ -f "model_download.py" ]; then
        python model_download.py download --model $model_name
    else
        log_warn "model_download.pyä¸å­˜åœ¨ï¼Œä½¿ç”¨huggingface-cliä¸‹è½½"
        model_id="Qwen/${model_name^}-Instruct"
        huggingface-cli download $model_id --local-dir /data/models/${model_name^}-Instruct
    fi
    
    log_info "æ¨¡å‹ä¸‹è½½å®Œæˆ"
}

# é…ç½®é˜²ç«å¢™
configure_firewall() {
    log_step "é…ç½®é˜²ç«å¢™..."
    
    # æ£€æŸ¥é˜²ç«å¢™çŠ¶æ€
    if command -v ufw &> /dev/null; then
        sudo ufw allow 8000/tcp
        log_info "UFWé˜²ç«å¢™è§„åˆ™å·²æ·»åŠ "
    elif command -v firewall-cmd &> /dev/null; then
        sudo firewall-cmd --permanent --add-port=8000/tcp
        sudo firewall-cmd --reload
        log_info "FirewallDé˜²ç«å¢™è§„åˆ™å·²æ·»åŠ "
    else
        log_warn "æœªæ£€æµ‹åˆ°é˜²ç«å¢™ç®¡ç†å·¥å…·"
    fi
}

# åˆ›å»ºsystemdæœåŠ¡
create_systemd_service() {
    log_step "åˆ›å»ºsystemdæœåŠ¡..."
    
    current_dir=$(pwd)
    current_user=$(whoami)
    
    cat > llm-server.service << EOF
[Unit]
Description=LLM Model Server
After=network.target

[Service]
Type=simple
User=$current_user
WorkingDirectory=$current_dir
Environment=PATH=$current_dir/venv/bin
Environment=PYTHONPATH=$current_dir
ExecStart=$current_dir/venv/bin/python vllm_server.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF
    
    sudo mv llm-server.service /etc/systemd/system/
    sudo systemctl daemon-reload
    
    log_info "systemdæœåŠ¡æ–‡ä»¶åˆ›å»ºå®Œæˆ"
}

# æµ‹è¯•æœåŠ¡
test_service() {
    log_step "æµ‹è¯•æœåŠ¡..."
    
    # æ£€æŸ¥æœåŠ¡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if [ ! -f "vllm_server.py" ]; then
        log_warn "vllm_server.pyä¸å­˜åœ¨ï¼Œè·³è¿‡æœåŠ¡æµ‹è¯•"
        return
    fi
    
    # å¯åŠ¨æœåŠ¡è¿›è¡Œæµ‹è¯•
    log_info "å¯åŠ¨æœåŠ¡è¿›è¡Œæµ‹è¯•..."
    python vllm_server.py &
    server_pid=$!
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    sleep 15
    
    # æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£
    if curl -s http://localhost:8000/health > /dev/null; then
        log_info "âœ… æœåŠ¡å¯åŠ¨æˆåŠŸï¼"
        echo "å¥åº·æ£€æŸ¥: http://localhost:8000/health"
        echo "APIæ–‡æ¡£: http://localhost:8000/docs"
    else
        log_warn "âš ï¸ æœåŠ¡å¯èƒ½å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"
    fi
    
    # åœæ­¢æµ‹è¯•æœåŠ¡
    kill $server_pid 2>/dev/null || true
    sleep 2
}

# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
show_usage() {
    echo "=========================================="
    log_info "éƒ¨ç½²å®Œæˆï¼"
    echo "=========================================="
    echo ""
    echo "ğŸš€ æœåŠ¡ç®¡ç†å‘½ä»¤:"
    echo "  å¯åŠ¨æœåŠ¡: sudo systemctl start llm-server"
    echo "  åœæ­¢æœåŠ¡: sudo systemctl stop llm-server"
    echo "  é‡å¯æœåŠ¡: sudo systemctl restart llm-server"
    echo "  æŸ¥çœ‹çŠ¶æ€: sudo systemctl status llm-server"
    echo "  æŸ¥çœ‹æ—¥å¿—: sudo journalctl -u llm-server -f"
    echo "  å¼€æœºè‡ªå¯: sudo systemctl enable llm-server"
    echo ""
    echo "ğŸŒ APIè®¿é—®åœ°å€:"
    echo "  å¥åº·æ£€æŸ¥: http://$(hostname -I | awk '{print $1}'):8000/health"
    echo "  APIæ–‡æ¡£: http://$(hostname -I | awk '{print $1}'):8000/docs"
    echo "  èŠå¤©æ¥å£: http://$(hostname -I | awk '{print $1}'):8000/chat"
    echo ""
    echo "ğŸ“ æµ‹è¯•å‘½ä»¤:"
    echo "  curl http://$(hostname -I | awk '{print $1}'):8000/health"
    echo "  python test_client.py"
    echo ""
    echo "âš ï¸  æ³¨æ„äº‹é¡¹:"
    echo "  - é¦–æ¬¡å¯åŠ¨å¯èƒ½éœ€è¦5-10åˆ†é’ŸåŠ è½½æ¨¡å‹"
    echo "  - å»ºè®®åœ¨screenæˆ–tmuxä¸­è¿è¡Œä»¥ä¿æŒæœåŠ¡ç¨³å®š"
    echo "  - å®šæœŸæ£€æŸ¥æ—¥å¿—å’Œç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"
    echo "  - å¦‚éœ€ä¿®æ”¹é…ç½®ï¼Œè¯·ç¼–è¾‘config.pyåé‡å¯æœåŠ¡"
    echo ""
}

# ä¸»å‡½æ•°
main() {
    log_info "å¼€å§‹è¿œç¨‹æœåŠ¡å™¨LLMå¤§æ¨¡å‹éƒ¨ç½²..."
    
    check_root
    detect_os
    check_hardware
    install_system_deps
    setup_python_env
    install_python_deps
    create_directories
    setup_huggingface_auth
    download_model
    configure_firewall
    create_systemd_service
    test_service
    show_usage
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"
